{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия №3\n",
    "<center>Автор материала: программист-исследователь Mail.Ru Group Юрий Кашницкий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашнее задание № 8\n",
    "## <center> Vowpal Wabbit в задаче классификации тегов вопросов на Stackoverflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План\n",
    "    1. Введение\n",
    "    2. Описание данных\n",
    "    3. Предобработка данных\n",
    "    4. Обучение и проверка моделей\n",
    "    5. Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Введение\n",
    "\n",
    "В этом задании вы будете делать примерно то же, что я каждую неделю –  в Mail.Ru Group: обучать модели на выборке в несколько гигабайт. Задание можно выполнить и на Windows с Python, но я рекомендую поработать под \\*NIX-системой (например, через Docker) и активно использовать язык bash.\n",
    "Немного снобизма (простите, но правда): если вы захотите работать в лучших компаниях мира в области ML, вам все равно понадобится опыт работы с bash под UNIX.\n",
    "\n",
    "[Веб-форма](https://docs.google.com/forms/d/1VaxYXnmbpeP185qPk2_V_BzbeduVUVyTdLPQwSCxDGA/edit) для ответов.\n",
    "\n",
    "Для выполнения задания понадобится установленный Vowpal Wabbit (уже есть в докер-контейнере курса, см. инструкцию в Wiki [репозитория](https://github.com/Yorko/mlcourse_open) нашего курса) и примерно 70 Гб дискового пространства. Я тестировал решение не на каком-то суперкомпе, а на Macbook Pro 2015 (8 ядер, 16 Гб памяти), и самая тяжеловесная модель обучалась около 12 минут, так что задание реально выполнить и с простым железом. Но если вы планируете когда-либо арендовать сервера Amazon, можно попробовать это сделать уже сейчас.\n",
    "\n",
    "Материалы в помощь:\n",
    " - интерактивный [тьюториал](https://www.codecademy.com/en/courses/learn-the-command-line/lessons/environment/exercises/bash-profile) CodeAcademy по утилитам командной строки UNIX (примерно на час-полтора)\n",
    " - [статья](https://habrahabr.ru/post/280562/) про то, как арендовать на Amazon машину (еще раз: это не обязательно для выполнения задания, но будет хорошим опытом, если вы это делаете впервые)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Описание данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеются 10 Гб вопросов со StackOverflow – [скачайте](https://drive.google.com/file/d/1ZU4J3KhJDrHVMj48fROFcTsTZKorPGlG/view) и распакуйте архив. \n",
    "\n",
    "Формат данных простой:<br>\n",
    "<center>*текст вопроса* (слова через пробел) TAB *теги вопроса* (через пробел)\n",
    "\n",
    "Здесь TAB – это символ табуляции.\n",
    "Пример первой записи в выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is there a way to apply a background color through css at the tr level i can apply it at the td level like this my td background color e8e8e8 background e8e8e8 however the background color doesn t seem to get applied when i attempt to apply the background color at the tr level like this my tr background color e8e8e8 background e8e8e8 is there a css trick to making this work or does css not natively support this for some reason \tcss css3 css-selectors\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 ../../data_external/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь у нас текст вопроса, затем табуляция и теги вопроса: *css, css3* и *css-selectors*. Всего в выборке таких вопросов 10 миллионов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 ../../data_external/stackoverflow.10kk.tsv\n",
      "CPU times: user 52 ms, sys: 8 ms, total: 60 ms\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wc -l ../../data_external/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на то, что такие данные я уже не хочу загружать в оперативную память и, пока можно, буду пользоваться эффективными утилитами UNIX –  head, tail, wc, cat, cut и прочими."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 1,7d ../../data_external/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i need to write a web application which works only with mysql database but all actual data lies within an oracle so now i m looking for some way to syncronize partially or use some automated tools to do that or just for a best practice for that case to be clear there s no way to use oracle directly from web applications due to security policies any advices \tmysql oracle\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 ../../data_external/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте выберем в наших данных все вопросы с тегами *javascript, java, python, ruby, php, c++, c#, go, scala* и  *swift* и подготовим обучающую выборку в формате Vowpal Wabbit. Будем решать задачу 10-классовой классификации вопросов по перечисленным тегам.\n",
    "\n",
    "Вообще, как мы видим, у каждого вопроса может быть несколько тегов, но мы упростим себе задачу и будем у каждого вопроса выбирать один из перечисленных тегов либо игнорировать вопрос, если таковых тегов нет. \n",
    "Но вообще VW поддерживает multilabel classification (аргумент  --multilabel_oaa).\n",
    "<br>\n",
    "<br>\n",
    "Реализуйте в виде отдельного файла `preprocess.py` код для подготовки данных. Он должен отобрать строки, в которых есть перечисленные теги, и переписать их в отдельный файл в формат Vowpal Wabbit. Детали:\n",
    " - скрипт должен работать с аргументами командной строки: с путями к файлам на входе и на выходе\n",
    " - строки обрабатываются по одной (можно использовать tqdm для подсчета числа итераций)\n",
    " - если табуляций в строке нет или их больше одной, считаем строку поврежденной и пропускаем\n",
    " - в противном случае смотрим, сколько в строке тегов из списка *javascript, java, python, ruby, php, c++, c#, go, scala* и  *swift*. Если ровно один, то записываем строку в выходной файл в формате VW: `label | text`, где `label` – число от 1 до 10 (1 - *javascript*, ... 10 – *swift*). Пропускаем те строки, где интересующих тегов больше или меньше одного \n",
    " - из текста вопроса надо выкинуть двоеточия и вертикальные палки, если они есть – в VW это спецсимволы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['javascript', 'java', 'python', 'ruby', 'php', 'c++', 'c#', 'go', 'scala', 'swift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = \"../../data_external/stackoverflow.10kk.tsv\"\n",
    "reader = open(fin, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | i am new to this forum and also to ganglia we are trying to capture jvm metrics through ganglia i have followed the steps through this link https github com ganglia jmxetric high level installation unzip the archive add the following to your jvm java javaagent lt path gt jmxetric jar host port config process usual java main class demo quickstart this example works for version 1 0 6 of jmxetric and gmetric4j ensure you have a gmond running on localhost 8649 pgrep gmond should return a valid pid nc localhost 8649 dumps some xml to stdout git clone same link as above download jmxetric jar gmetric4j jar and oncrpc 1 0 7 jar all into the same directory cd jmxetric in bash do export config host localhost port 8649 wireformat31x true config etc jmxetric xml java djava util logging config file etc logging properties cp gmetric4j 1 0 6 jar oncrpc 1 0 7 jar jmxetric 1 0 6 jar javaagent jmxetric 1 0 4 jar config info ganglia jmxetric jmxetricagent but when i add the jvm argument in was its not coming up and native sterr logs shows jmxetricagent instrumented jvm see same link what i have used test wakeup and then in around every 10 mints it keeps on writing test wakeup but instance does not starts also i have tried to run java command through command line like this root hostname var java javaagent etc ganglia jmxmetric jmxetric master jmxetric 1 0 6 jar host hostname port 8649 mode unicast wireformat31x true config etc ganglia jmxmetric jmxetric master jmxetric xml process hostanem_m01 info ganglia jmxetric jmxetricagent getting same response jmxetricagent instrumented jvm test wakeup can you all please suggest as what i am doing wrong here thanks much in advance its been a week i am struggling with this\n",
      "\n"
     ]
    }
   ],
   "source": [
    "line = reader.readline()\n",
    "text_labels = line[:-1].split(\"\\t\")\n",
    "if len(text_labels) != 2:\n",
    "#     n_corrupted_lines += 1\n",
    "    raise Exception(\"Corrupted\")\n",
    "text, labels = text_labels\n",
    "labels = labels.split(' ')\n",
    "found_tags = [l for l in labels if l in tags]\n",
    "if len(found_tags) != 1:\n",
    "    raise Exception(\"No tags : {}\".format(found_tags))\n",
    "text = text.strip().replace(':', ' ').replace('|',' ')\n",
    "label = tags.index(found_tags[0])\n",
    "print(\"{} | {}\\n\".format(label, text))\n",
    "# n_written_lines += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 | i have a collection of objects that are guaranteed to be distinct in particular indexed by a unique integer id i also know exactly how many of them there are and the number won t change and was wondering whether array would have a notable performance advantage over hashset for storing retrieving said elements on paper array guarantees constant time insertion since i know the size ahead of time and retrieval but the code for hashset looks much cleaner and adds some flexibility so i m wondering if i m losing anything performance wise using it at least theoretically'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.strip().replace(':', ' ').replace('|',' ')\n",
    "label = tags.index(found_tags[0])\n",
    "\"{} | {}\".format(label, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tags = ['javascript', 'java', 'python', 'ruby', 'php', 'c++', 'c#', 'go', 'scala', 'swift']\n",
    "\n",
    "def main(fin, fout):\n",
    "    \n",
    "    n_corrupted_lines = 0\n",
    "    n_written_lines = 0\n",
    "    \n",
    "    with tqdm(total=9999994) as pbar:\n",
    "        with open(fin, 'r') as reader:\n",
    "            with open(fout, 'bw') as writer:\n",
    "                while True:\n",
    "                    line = reader.readline()\n",
    "                    pbar.update(1)                    \n",
    "                    if len(line) == 0:\n",
    "                        break\n",
    "                    text_labels = line[:-1].split(\"\\t\")\n",
    "                    if len(text_labels) != 2:\n",
    "                        n_corrupted_lines += 1\n",
    "                        continue\n",
    "                    text, labels = text_labels\n",
    "                    labels = labels.split(' ')\n",
    "                    found_tags = [l for l in labels if l in tags]\n",
    "                    if len(found_tags) != 1:\n",
    "                        continue\n",
    "                    text = text.strip().replace(':', ' ').replace('|',' ')\n",
    "                    label = tags.index(found_tags[0]) + 1\n",
    "                    writer.write(\"{} | {}\\n\".format(label, text).encode('utf-8'))\n",
    "                    n_written_lines += 1\n",
    "    print(\"{} lines selected, {} lines corrupted.\".format(n_written_lines, n_corrupted_lines))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fin = sys.argv[1]\n",
    "    fout = sys.argv[2]\n",
    "    assert os.path.exists(fin)\n",
    "    if fin is None or fout is None:\n",
    "        print('Incorrect args:\\nfin - {}\\nfout - {}'.format(fin, fout))\n",
    "        sys.exit(-1)\n",
    "    main(fin, fout)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m py_compile preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Должно получиться вот такое число строк – 4389054. 10 Гб у меня обработались примерно за 2 минуты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|#############################| 9999994/9999994 [00:58<00:00, 170925.14it/s]\n",
      "4389056 lines selected, 0 lines corrupted.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!python3 preprocess.py ../../data_external/stackoverflow.10kk.tsv ../../data_external/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4389056 ../../data_external/stackoverflow.vw\n",
      "CPU times: user 28 ms, sys: 8 ms, total: 36 ms\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wc -l ../../data_external/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gzip ../../data_external/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поделите выборку на обучающую, проверочную и тестовую части в равной пропорции - по  1463018 в каждый файл. Перемешивать не надо, первые 1463018 строк должны пойти в обучающую часть `stackoverflow_train.vw`, последние 1463018 – в тестовую `stackoverflow_test.vw`, оставшиеся – в проверочную `stackoverflow_valid.vw`. \n",
    "\n",
    "Также сохраните векторы ответов для проверочной и тестовой выборки в отдельные файлы `stackoverflow_valid_labels.txt` и `stackoverflow_test_labels.txt`.\n",
    "\n",
    "Тут вам помогут утилиты `head`, `tail`, `split`, `cat` и `cut`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: split [OPTION]... [FILE [PREFIX]]\r\n",
      "Output pieces of FILE to PREFIXaa, PREFIXab, ...;\r\n",
      "default size is 1000 lines, and default PREFIX is 'x'.\r\n",
      "\r\n",
      "With no FILE, or when FILE is -, read standard input.\r\n",
      "\r\n",
      "Mandatory arguments to long options are mandatory for short options too.\r\n",
      "  -a, --suffix-length=N   generate suffixes of length N (default 2)\r\n",
      "      --additional-suffix=SUFFIX  append an additional SUFFIX to file names\r\n",
      "  -b, --bytes=SIZE        put SIZE bytes per output file\r\n",
      "  -C, --line-bytes=SIZE   put at most SIZE bytes of records per output file\r\n",
      "  -d                      use numeric suffixes starting at 0, not alphabetic\r\n",
      "      --numeric-suffixes[=FROM]  same as -d, but allow setting the start value\r\n",
      "  -e, --elide-empty-files  do not generate empty output files with '-n'\r\n",
      "      --filter=COMMAND    write to shell COMMAND; file name is $FILE\r\n",
      "  -l, --lines=NUMBER      put NUMBER lines/records per output file\r\n",
      "  -n, --number=CHUNKS     generate CHUNKS output files; see explanation below\r\n",
      "  -t, --separator=SEP     use SEP instead of newline as the record separator;\r\n",
      "                            '\\0' (zero) specifies the NUL character\r\n",
      "  -u, --unbuffered        immediately copy input to output with '-n r/...'\r\n",
      "      --verbose           print a diagnostic just before each\r\n",
      "                            output file is opened\r\n",
      "      --help     display this help and exit\r\n",
      "      --version  output version information and exit\r\n",
      "\r\n",
      "The SIZE argument is an integer and optional unit (example: 10K is 10*1024).\r\n",
      "Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).\r\n",
      "\r\n",
      "CHUNKS may be:\r\n",
      "  N       split into N files based on size of input\r\n",
      "  K/N     output Kth of N to stdout\r\n",
      "  l/N     split into N files without splitting lines/records\r\n",
      "  l/K/N   output Kth of N to stdout without splitting lines/records\r\n",
      "  r/N     like 'l' but use round robin distribution\r\n",
      "  r/K/N   likewise but only output Kth of N to stdout\r\n",
      "\r\n",
      "GNU coreutils online help: <http://www.gnu.org/software/coreutils/>\r\n",
      "Report split translation bugs to <http://translationproject.org/team/>\r\n",
      "Full documentation at: <http://www.gnu.org/software/coreutils/split>\r\n",
      "or available locally via: info '(coreutils) split invocation'\r\n"
     ]
    }
   ],
   "source": [
    "!split --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating file '../../data_external/stackoverflow_aa.vw'\n",
      "creating file '../../data_external/stackoverflow_ab.vw'\n",
      "creating file '../../data_external/stackoverflow_ac.vw'\n",
      "creating file '../../data_external/stackoverflow_ad.vw'\n"
     ]
    }
   ],
   "source": [
    "!split -l 1463018 --verbose --additional-suffix=.vw ../../data_external/stackoverflow.vw ../../data_external/stackoverflow_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ../../data_external/stackoverflow_aa.vw ../../data_external/stackoverflow_train.vw\n",
    "!mv ../../data_external/stackoverflow_ab.vw ../../data_external/stackoverflow_valid.vw\n",
    "!mv ../../data_external/stackoverflow_ac.vw ../../data_external/stackoverflow_test.vw\n",
    "!cat ../../data_external/stackoverflow_ad.vw >> ../../data_external/stackoverflow_test.vw\n",
    "!rm ../../data_external/stackoverflow_ad.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1463020 ../../data_external/stackoverflow_test.vw\n",
      "   1463018 ../../data_external/stackoverflow_train.vw\n",
      "   1463018 ../../data_external/stackoverflow_valid.vw\n",
      "   4389056 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../../data_external/stackoverflow_*.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !du -h -a -d 1 ../../data_external/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: cut OPTION... [FILE]...\r\n",
      "Print selected parts of lines from each FILE to standard output.\r\n",
      "\r\n",
      "With no FILE, or when FILE is -, read standard input.\r\n",
      "\r\n",
      "Mandatory arguments to long options are mandatory for short options too.\r\n",
      "  -b, --bytes=LIST        select only these bytes\r\n",
      "  -c, --characters=LIST   select only these characters\r\n",
      "  -d, --delimiter=DELIM   use DELIM instead of TAB for field delimiter\r\n",
      "  -f, --fields=LIST       select only these fields;  also print any line\r\n",
      "                            that contains no delimiter character, unless\r\n",
      "                            the -s option is specified\r\n",
      "  -n                      (ignored)\r\n",
      "      --complement        complement the set of selected bytes, characters\r\n",
      "                            or fields\r\n",
      "  -s, --only-delimited    do not print lines not containing delimiters\r\n",
      "      --output-delimiter=STRING  use STRING as the output delimiter\r\n",
      "                            the default is to use the input delimiter\r\n",
      "  -z, --zero-terminated    line delimiter is NUL, not newline\r\n",
      "      --help     display this help and exit\r\n",
      "      --version  output version information and exit\r\n",
      "\r\n",
      "Use one, and only one of -b, -c or -f.  Each LIST is made up of one\r\n",
      "range, or many ranges separated by commas.  Selected input is written\r\n",
      "in the same order that it is read, and is written exactly once.\r\n",
      "Each range is one of:\r\n",
      "\r\n",
      "  N     N'th byte, character or field, counted from 1\r\n",
      "  N-    from N'th byte, character or field, to end of line\r\n",
      "  N-M   from N'th to M'th (included) byte, character or field\r\n",
      "  -M    from first to M'th (included) byte, character or field\r\n",
      "\r\n",
      "GNU coreutils online help: <http://www.gnu.org/software/coreutils/>\r\n",
      "Report cut translation bugs to <http://translationproject.org/team/>\r\n",
      "Full documentation at: <http://www.gnu.org/software/coreutils/cut>\r\n",
      "or available locally via: info '(coreutils) cut invocation'\r\n"
     ]
    }
   ],
   "source": [
    "!cut --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../../data_external/stackoverflow_valid.vw | cut -d\\| -f 1 > ../../data_external/stackoverflow_valid_labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../../data_external/stackoverflow_test.vw | cut -d\\| -f 1 > ../../data_external/stackoverflow_test_labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 \r\n",
      "5 \r\n",
      "6 \r\n",
      "1 \r\n",
      "7 \r\n",
      "7 \r\n",
      "7 \r\n",
      "4 \r\n",
      "5 \r\n",
      "3 \r\n"
     ]
    }
   ],
   "source": [
    "!head ../../data_external/stackoverflow_valid_labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 \r\n",
      "2 \r\n",
      "9 \r\n",
      "1 \r\n",
      "7 \r\n",
      "5 \r\n",
      "7 \r\n",
      "9 \r\n",
      "7 \r\n",
      "2 \r\n"
     ]
    }
   ],
   "source": [
    "!head ../../data_external/stackoverflow_test_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Обучение и проверка моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите Vowpal Wabbit на выборке `stackoverflow_train.vw` 9 раз, перебирая параметры passes (1,3,5), ngram (1,2,3).\n",
    "Остальные параметры укажите следующие: bit_precision=28 и seed=17. Также скажите VW, что это 10-классовая задача.\n",
    "\n",
    "Проверяйте долю правильных ответов на выборке `stackoverflow_valid.vw`. Выберите лучшую модель и проверьте качество на выборке `stackoverflow_test.vw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    \"--passes 1 --ngram 1\",\n",
    "    \"--passes 1 --ngram 2\",\n",
    "    \"--passes 1 --ngram 3\",\n",
    "    \n",
    "    \"--passes 3 --ngram 1\",\n",
    "    \"--passes 3 --ngram 2\",\n",
    "    \"--passes 3 --ngram 3\",\n",
    "    \n",
    "    \"--passes 5 --ngram 1\",\n",
    "    \"--passes 5 --ngram 2\",\n",
    "    \"--passes 5 --ngram 3\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw1_session3_data_analysis_pandas.ipynb\r\n",
      "hw2_session3_visual_data_analysis.ipynb\r\n",
      "hw3_session3_decision_trees.ipynb\r\n",
      "hw3_session3_decision_trees_solution.ipynb\r\n",
      "hw3_session3_optional_implement_dt.ipynb\r\n",
      "hw4_session3_stackoverflow_logistic_regression.ipynb\r\n",
      "hw5_session3_rf_logit_scoring_texts.ipynb\r\n",
      "hw6\r\n",
      "hw6_session3_part1_alice_beat_baseline.ipynb\r\n",
      "hw6_session3_part1_alice_beat_baseline_RF.ipynb\r\n",
      "hw6_session3_part1_alice_beat_baseline_basics.ipynb\r\n",
      "hw6_session3_part1_alice_beat_baseline_tsfresh.ipynb\r\n",
      "hw6_session3_part2_medium_beat_baseline.ipynb\r\n",
      "hw7_session3_unsupervised_learning.ipynb\r\n",
      "hw8_session3_vw_stackoverflow_tags_10mln.ipynb\r\n",
      "preprocess.py\r\n",
      "preprocess.pyc\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "--passes 1 --ngram 1\n",
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = vw_model_0.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using cache_file = ../../data_external/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1       59\n",
      "0.500000 1.000000            2            2.0        7        1       88\n",
      "0.750000 1.000000            4            4.0        1        7      143\n",
      "0.875000 1.000000            8            8.0        5        7      446\n",
      "0.875000 0.875000           16           16.0        6        7      331\n",
      "0.812500 0.750000           32           32.0        2        7      236\n",
      "0.765625 0.718750           64           64.0        1        1      288\n",
      "0.687500 0.609375          128          128.0        5        1      151\n",
      "0.609375 0.531250          256          256.0        2        2      167\n",
      "0.552734 0.496094          512          512.0        5        5      146\n",
      "0.455078 0.357422         1024         1024.0        2        7      210\n",
      "0.376953 0.298828         2048         2048.0        2        7       43\n",
      "0.314941 0.252930         4096         4096.0        2        7      179\n",
      "0.270020 0.225098         8192         8192.0        2        2       38\n",
      "0.228394 0.186768        16384        16384.0        7        2      169\n",
      "0.189545 0.150696        32768        32768.0        3        3      430\n",
      "0.162903 0.136261        65536        65536.0        3        3      103\n",
      "0.141487 0.120071       131072       131072.0        5        5       71\n",
      "0.123783 0.106079       262144       262144.0        7        7      161\n",
      "0.112253 0.100723       524288       524288.0        1        7       14\n",
      "0.102877 0.093500      1048576      1048576.0        1        1      151\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.098719\n",
      "total feature number = 291954713\n",
      "Accuracy:  0.9151500528359869\n",
      "\n",
      "\n",
      "\n",
      "--passes 1 --ngram 2\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = vw_model_1.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using cache_file = ../../data_external/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      116\n",
      "0.500000 1.000000            2            2.0        7        1      174\n",
      "0.750000 1.000000            4            4.0        1        7      284\n",
      "0.875000 1.000000            8            8.0        5        7      890\n",
      "0.875000 0.875000           16           16.0        6        7      660\n",
      "0.781250 0.687500           32           32.0        2        2      470\n",
      "0.765625 0.750000           64           64.0        1        1      574\n",
      "0.671875 0.578125          128          128.0        5        1      300\n",
      "0.597656 0.523438          256          256.0        2        2      332\n",
      "0.527344 0.457031          512          512.0        5        5      290\n",
      "0.432617 0.337891         1024         1024.0        2        7      418\n",
      "0.361816 0.291016         2048         2048.0        2        2       84\n",
      "0.298584 0.235352         4096         4096.0        2        7      356\n",
      "0.250122 0.201660         8192         8192.0        2        2       74\n",
      "0.210205 0.170288        16384        16384.0        7        2      336\n",
      "0.175446 0.140686        32768        32768.0        3        3      858\n",
      "0.148209 0.120972        65536        65536.0        3        3      204\n",
      "0.127487 0.106766       131072       131072.0        5        5      140\n",
      "0.109516 0.091545       262144       262144.0        7        7      320\n",
      "0.097635 0.085754       524288       524288.0        1        2       26\n",
      "0.087303 0.076971      1048576      1048576.0        1        1      300\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.083257\n",
      "total feature number = 580983390\n",
      "Accuracy:  0.9309892291140642\n",
      "\n",
      "\n",
      "\n",
      "--passes 1 --ngram 3\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = vw_model_2.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using cache_file = ../../data_external/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      172\n",
      "0.500000 1.000000            2            2.0        7        1      259\n",
      "0.750000 1.000000            4            4.0        1        7      424\n",
      "0.875000 1.000000            8            8.0        5        7     1333\n",
      "0.875000 0.875000           16           16.0        6        7      988\n",
      "0.843750 0.812500           32           32.0        2        7      703\n",
      "0.796875 0.750000           64           64.0        1        1      859\n",
      "0.671875 0.546875          128          128.0        5        1      448\n",
      "0.621094 0.570312          256          256.0        2        1      496\n",
      "0.539062 0.457031          512          512.0        5        5      433\n",
      "0.450195 0.361328         1024         1024.0        2        7      625\n",
      "0.383789 0.317383         2048         2048.0        2        2      124\n",
      "0.319824 0.255859         4096         4096.0        2        7      532\n",
      "0.267456 0.215088         8192         8192.0        2        2      109\n",
      "0.222595 0.177734        16384        16384.0        7        2      502\n",
      "0.184570 0.146545        32768        32768.0        3        3     1285\n",
      "0.155487 0.126404        65536        65536.0        3        3      304\n",
      "0.132759 0.110031       131072       131072.0        5        5      208\n",
      "0.113411 0.094063       262144       262144.0        7        7      478\n",
      "0.101025 0.088638       524288       524288.0        1        7       37\n",
      "0.089794 0.078564      1048576      1048576.0        1        1      448\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.085607\n",
      "total feature number = 868549054\n",
      "Accuracy:  0.9293009381976162\n",
      "\n",
      "\n",
      "\n",
      "--passes 3 --ngram 1\n",
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = vw_model_3.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = ../../data_external/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1       59\n",
      "0.500000 1.000000            2            2.0        7        1       88\n",
      "0.750000 1.000000            4            4.0        1        7      143\n",
      "0.875000 1.000000            8            8.0        5        7      446\n",
      "0.812500 0.750000           16           16.0        2        2       79\n",
      "0.781250 0.750000           32           32.0        5        2       60\n",
      "0.750000 0.718750           64           64.0        2        5      137\n",
      "0.695312 0.640625          128          128.0        7        6      180\n",
      "0.613281 0.531250          256          256.0        1        7       99\n",
      "0.556641 0.500000          512          512.0        6        2      156\n",
      "0.463867 0.371094         1024         1024.0        3        3      135\n",
      "0.379883 0.295898         2048         2048.0        1        1       41\n",
      "0.309570 0.239258         4096         4096.0        3        3       61\n",
      "0.267212 0.224854         8192         8192.0        2        2      109\n",
      "0.227905 0.188599        16384        16384.0        4        6       52\n",
      "0.191101 0.154297        32768        32768.0        1        1       53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.162598 0.134094        65536        65536.0        7        3       61\n",
      "0.141228 0.119858       131072       131072.0        7        7       80\n",
      "0.124710 0.108192       262144       262144.0        5        5      158\n",
      "0.112028 0.099346       524288       524288.0        7        7      149\n",
      "0.103410 0.094791      1048576      1048576.0        3        3      152\n",
      "0.095728 0.095728      2097152      2097152.0        7        7       42 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 3\n",
      "weighted example sum = 3950151.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.087621 h\n",
      "total feature number = 787979784\n",
      "Accuracy:  0.9142737820040492\n",
      "\n",
      "\n",
      "\n",
      "--passes 3 --ngram 2\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = vw_model_4.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = ../../data_external/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      116\n",
      "0.500000 1.000000            2            2.0        7        1      174\n",
      "0.750000 1.000000            4            4.0        1        7      284\n",
      "0.875000 1.000000            8            8.0        5        7      890\n",
      "0.750000 0.625000           16           16.0        2        2      156\n",
      "0.750000 0.750000           32           32.0        5        1      118\n",
      "0.703125 0.656250           64           64.0        2        5      272\n",
      "0.640625 0.578125          128          128.0        7        6      358\n",
      "0.574219 0.507812          256          256.0        1        7      196\n",
      "0.523438 0.472656          512          512.0        6        6      310\n",
      "0.443359 0.363281         1024         1024.0        3        3      268\n",
      "0.365723 0.288086         2048         2048.0        1        1       80\n",
      "0.296875 0.228027         4096         4096.0        3        3      120\n",
      "0.250000 0.203125         8192         8192.0        2        2      216\n",
      "0.209595 0.169189        16384        16384.0        4        6      102\n",
      "0.175323 0.141052        32768        32768.0        1        1      104\n",
      "0.146820 0.118317        65536        65536.0        7        1      120\n",
      "0.126236 0.105652       131072       131072.0        7        7      158\n",
      "0.110115 0.093994       262144       262144.0        5        5      314\n",
      "0.096981 0.083847       524288       524288.0        7        7      296\n",
      "0.087324 0.077667      1048576      1048576.0        3        3      302\n",
      "0.080300 0.080300      2097152      2097152.0        7        7       82 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 3\n",
      "weighted example sum = 3950151.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.072488 h\n",
      "total feature number = 1568059266\n",
      "Accuracy:  0.9282961658708232\n",
      "\n",
      "\n",
      "\n",
      "--passes 3 --ngram 3\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = vw_model_5.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = ../../data_external/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      172\n",
      "0.500000 1.000000            2            2.0        7        1      259\n",
      "0.750000 1.000000            4            4.0        1        7      424\n",
      "0.875000 1.000000            8            8.0        5        7     1333\n",
      "0.812500 0.750000           16           16.0        2        2      232\n",
      "0.812500 0.812500           32           32.0        5        1      175\n",
      "0.734375 0.656250           64           64.0        2        5      406\n",
      "0.648438 0.562500          128          128.0        7        6      535\n",
      "0.582031 0.515625          256          256.0        1        7      292\n",
      "0.531250 0.480469          512          512.0        6        6      463\n",
      "0.451172 0.371094         1024         1024.0        3        3      400\n",
      "0.380371 0.309570         2048         2048.0        1        1      118\n",
      "0.312256 0.244141         4096         4096.0        3        6      178\n",
      "0.264893 0.217529         8192         8192.0        2        2      322\n",
      "0.223877 0.182861        16384        16384.0        4        6      151\n",
      "0.186462 0.149048        32768        32768.0        1        1      154\n",
      "0.155731 0.125000        65536        65536.0        7        1      178\n",
      "0.132889 0.110046       131072       131072.0        7        7      235\n",
      "0.115025 0.097160       262144       262144.0        5        5      469\n",
      "0.100910 0.086796       524288       524288.0        7        7      442\n",
      "0.090398 0.079885      1048576      1048576.0        3        3      451\n",
      "0.082746 0.082746      2097152      2097152.0        7        7      121 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 3\n",
      "weighted example sum = 3950151.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.074258 h\n",
      "total feature number = 2344188606\n",
      "Accuracy:  0.9244828156591375\n",
      "\n",
      "\n",
      "\n",
      "--passes 5 --ngram 1\n",
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = vw_model_6.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = ../../data_external/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1       59\n",
      "0.500000 1.000000            2            2.0        7        1       88\n",
      "0.750000 1.000000            4            4.0        1        7      143\n",
      "0.875000 1.000000            8            8.0        5        7      446\n",
      "0.812500 0.750000           16           16.0        2        2       79\n",
      "0.781250 0.750000           32           32.0        5        2       60\n",
      "0.750000 0.718750           64           64.0        2        5      137\n",
      "0.695312 0.640625          128          128.0        7        6      180\n",
      "0.613281 0.531250          256          256.0        1        7       99\n",
      "0.556641 0.500000          512          512.0        6        2      156\n",
      "0.463867 0.371094         1024         1024.0        3        3      135\n",
      "0.379883 0.295898         2048         2048.0        1        1       41\n",
      "0.309570 0.239258         4096         4096.0        3        3       61\n",
      "0.267212 0.224854         8192         8192.0        2        2      109\n",
      "0.227905 0.188599        16384        16384.0        4        6       52\n",
      "0.191101 0.154297        32768        32768.0        1        1       53\n",
      "0.162598 0.134094        65536        65536.0        7        3       61\n",
      "0.141228 0.119858       131072       131072.0        7        7       80\n",
      "0.124710 0.108192       262144       262144.0        5        5      158\n",
      "0.112028 0.099346       524288       524288.0        7        7      149\n",
      "0.103410 0.094791      1048576      1048576.0        3        3      152\n",
      "0.095728 0.095728      2097152      2097152.0        7        7       42 h\n",
      "0.091934 0.088140      4194304      4194304.0        6        6      105 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.087621 h\n",
      "total feature number = 1313299640\n",
      "Accuracy:  0.9133154889413527\n",
      "\n",
      "\n",
      "\n",
      "--passes 5 --ngram 2\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = vw_model_7.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = ../../data_external/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      116\n",
      "0.500000 1.000000            2            2.0        7        1      174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.750000 1.000000            4            4.0        1        7      284\n",
      "0.875000 1.000000            8            8.0        5        7      890\n",
      "0.750000 0.625000           16           16.0        2        2      156\n",
      "0.750000 0.750000           32           32.0        5        1      118\n",
      "0.703125 0.656250           64           64.0        2        5      272\n",
      "0.640625 0.578125          128          128.0        7        6      358\n",
      "0.574219 0.507812          256          256.0        1        7      196\n",
      "0.523438 0.472656          512          512.0        6        6      310\n",
      "0.443359 0.363281         1024         1024.0        3        3      268\n",
      "0.365723 0.288086         2048         2048.0        1        1       80\n",
      "0.296875 0.228027         4096         4096.0        3        3      120\n",
      "0.250000 0.203125         8192         8192.0        2        2      216\n",
      "0.209595 0.169189        16384        16384.0        4        6      102\n",
      "0.175323 0.141052        32768        32768.0        1        1      104\n",
      "0.146820 0.118317        65536        65536.0        7        1      120\n",
      "0.126236 0.105652       131072       131072.0        7        7      158\n",
      "0.110115 0.093994       262144       262144.0        5        5      314\n",
      "0.096981 0.083847       524288       524288.0        7        7      296\n",
      "0.087324 0.077667      1048576      1048576.0        3        3      302\n",
      "0.080300 0.080300      2097152      2097152.0        7        7       82 h\n",
      "0.076624 0.072948      4194304      4194304.0        6        6      208 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.072488 h\n",
      "total feature number = 2613432110\n",
      "Accuracy:  0.9261635878710993\n",
      "\n",
      "\n",
      "\n",
      "--passes 5 --ngram 3\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = vw_model_8.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = ../../data_external/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      172\n",
      "0.500000 1.000000            2            2.0        7        1      259\n",
      "0.750000 1.000000            4            4.0        1        7      424\n",
      "0.875000 1.000000            8            8.0        5        7     1333\n",
      "0.812500 0.750000           16           16.0        2        2      232\n",
      "0.812500 0.812500           32           32.0        5        1      175\n",
      "0.734375 0.656250           64           64.0        2        5      406\n",
      "0.648438 0.562500          128          128.0        7        6      535\n",
      "0.582031 0.515625          256          256.0        1        7      292\n",
      "0.531250 0.480469          512          512.0        6        6      463\n",
      "0.451172 0.371094         1024         1024.0        3        3      400\n",
      "0.380371 0.309570         2048         2048.0        1        1      118\n",
      "0.312256 0.244141         4096         4096.0        3        6      178\n",
      "0.264893 0.217529         8192         8192.0        2        2      322\n",
      "0.223877 0.182861        16384        16384.0        4        6      151\n",
      "0.186462 0.149048        32768        32768.0        1        1      154\n",
      "0.155731 0.125000        65536        65536.0        7        1      178\n",
      "0.132889 0.110046       131072       131072.0        7        7      235\n",
      "0.115025 0.097160       262144       262144.0        5        5      469\n",
      "0.100910 0.086796       524288       524288.0        7        7      442\n",
      "0.090398 0.079885      1048576      1048576.0        3        3      451\n",
      "0.082746 0.082746      2097152      2097152.0        7        7      121 h\n",
      "0.079332 0.075918      4194304      4194304.0        6        6      310 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.074258 h\n",
      "total feature number = 3906981010\n",
      "Accuracy:  0.9263413027044096\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "for i, p in enumerate(params):\n",
    "    print(\"\\n\\n\")\n",
    "    print(p)\n",
    "    \n",
    "    !vw --oaa 10 -d ../../data_external/stackoverflow_train.vw -f vw_model_{i}.vw -b 28 --random_seed 17 -c {p}\n",
    "    !vw -t -i vw_model_{i}.vw -d ../../data_external/stackoverflow_valid.vw -p vw_model_{i}_pred.csv --random_seed 17 --quiet\n",
    "    \n",
    "    vw_pred = np.loadtxt('vw_model_{}_pred.csv'.format(i))\n",
    "    test_labels = np.loadtxt('../../data_external/stackoverflow_valid_labels.txt')\n",
    "    print(\"Accuracy: \", accuracy_score(test_labels, vw_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Вопрос 1.</font> Какое сочетание параметров дает наибольшую долю правильных ответов на проверочной выборке `stackoverflow_valid.vw`?\n",
    "- Биграммы и 3 прохода по выборке\n",
    "- Триграммы и 5 проходов по выборке\n",
    "- Биграммы и 1 проход по выборке\n",
    "- Униграммы и 1 проход по выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ответ: 3\n",
    "# Биграммы и 1 проход по выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте лучшую (по доле правильных ответов на валидации) модель на тестовой выборке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9311321786441744\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "!vw -t -i vw_model_{i}.vw -d ../../data_external/stackoverflow_test.vw -p vw_model_{i}_test_pred.csv --random_seed 17 --quiet\n",
    "\n",
    "vw_pred = np.loadtxt('vw_model_{}_test_pred.csv'.format(i))\n",
    "test_labels = np.loadtxt('../../data_external/stackoverflow_test_labels.txt')\n",
    "print(\"Accuracy: \", accuracy_score(test_labels, vw_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Вопрос 2.</font> Как соотносятся доли правильных ответов лучшей (по доле правильных ответов на валидации) модели на проверочной и на тестовой выборках? (здесь % – это процентный пункт, т.е., скажем, снижение с 50% до 40% – это на 10%, а не 20%).\n",
    "- На тестовой ниже примерно на 2%\n",
    "- На тестовой ниже примерно на 3%\n",
    "- Результаты почти одинаковы – отличаются меньше чем на 0.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001429495301101813"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.9311321786441744 - 0.9309892291140642) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ответ : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите VW с параметрами, подобранными на проверочной выборке, теперь на объединении обучающей и проверочной выборок. Посчитайте долю правильных ответов на тестовой выборке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../../data_external/stackoverflow_train.vw ../../data_external/stackoverflow_trainval.vw\n",
    "!cat ../../data_external/stackoverflow_valid.vw >> ../../data_external/stackoverflow_trainval.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2926036 ../../data_external/stackoverflow_trainval.vw\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../../data_external/stackoverflow_trainval.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9352701945291246\n"
     ]
    }
   ],
   "source": [
    "p = \"--passes=1 --ngram=2\"\n",
    "!vw --oaa 10 -d ../../data_external/stackoverflow_trainval.vw -f vw_model_passes=1_ngram=2_trainval.vw -b 28 --random_seed 17 -c {p} --quiet\n",
    "!vw -t -i vw_model_passes=1_ngram=2_trainval.vw -d ../../data_external/stackoverflow_test.vw -p vw_model_passes=1_ngram=2_test_pred.csv --random_seed 17 --quiet\n",
    "\n",
    "vw_pred = np.loadtxt('vw_model_passes=1_ngram=2_test_pred.csv')\n",
    "test_labels = np.loadtxt('../../data_external/stackoverflow_test_labels.txt')\n",
    "print(\"Accuracy: \", accuracy_score(test_labels, vw_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Вопрос 3.</font> На сколько процентных пунктов повысилась доля правильных ответов модели после обучения на вдвое большей выборке (обучающая `stackoverflow_train.vw` + проверочная `stackoverflow_valid.vw`) по сравнению с моделью, обученной только на `stackoverflow_train.vw`?\n",
    " - 0.1%\n",
    " - 0.4%\n",
    " - 0.8%\n",
    " - 1.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42809654150604004"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.9352701945291246 - 0.9309892291140642)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ответ: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы только познакомились с Vowpal Wabbit. Что еще можно попробовать:\n",
    " - Классификация с несколькими ответами (multilabel classification, аргумент  `multilabel_oaa`) – формат данных тут как раз под такую задачу\n",
    " - Настройка параметров VW с hyperopt, авторы библиотеки утверждают, что качество должно сильно зависеть от параметров изменения шага градиентного спуска (`initial_t` и `power_t`). Также можно потестировать разные функции потерь – обучать логистическую регресиию или линейный SVM\n",
    " - Познакомиться с факторизационными машинами и их реализацией в VW (аргумент `lrq`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
